% Encoding: UTF-8

@Article{Mccarthy1976,
  author        = {Mccarthy, John},
  title         = {An example for natural language understanding and the AI problems it raises},
  year          = {1976},
  month         = {09},
  __markedentry = {[noambard:6]},
  file          = {:refs/mccarthy1976.pdf:PDF},
}

@Article{Hermann2015,
  author      = {Karl Moritz Hermann and Tomáš Kočiský and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman and Phil Blunsom},
  title       = {Teaching Machines to Read and Comprehend},
  abstract    = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
  date        = {2015-06-10},
  eprint      = {http://arxiv.org/abs/1506.03340v3},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1506.03340v3:PDF},
  keywords    = {cs.CL, cs.AI, cs.NE},
}

@Article{Rajpurkar2016,
  author      = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  title       = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  abstract    = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
  date        = {2016-06-16},
  eprint      = {http://arxiv.org/abs/1606.05250v3},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:Rajpurkar2016 - SQuAD_ 100,000+ Questions for Machine Comprehension of Text.pdf:PDF},
  keywords    = {cs.CL},
}

@Article{Trischler2016,
  author      = {Adam Trischler and Tong Wang and Xingdi Yuan and Justin Harris and Alessandro Sordoni and Philip Bachman and Kaheer Suleman},
  title       = {NewsQA: A Machine Comprehension Dataset},
  abstract    = {We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.},
  date        = {2016-11-29},
  eprint      = {http://arxiv.org/abs/1611.09830v3},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1611.09830v3:PDF},
  keywords    = {cs.CL, cs.AI},
}

@Article{Clark2016,
  author    = {Peter Clark and Oren Etzioni},
  title     = {My Computer Is an Honor Student {\textemdash} but How Intelligent Is It? Standardized Tests as a Measure of {AI}},
  journal   = {{AI} Magazine},
  year      = {2016},
  volume    = {37},
  number    = {1},
  pages     = {5},
  month     = {apr},
  doi       = {10.1609/aimag.v37i1.2636},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}

@InProceedings{N18-1023,
  author    = {Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  title     = {Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  year      = {2018},
  pages     = {252--262},
  publisher = {Association for Computational Linguistics},
  location  = {New Orleans, Louisiana},
  url       = {http://aclweb.org/anthology/N18-1023},
}

@Article{Rajpurkar2018,
  author      = {Pranav Rajpurkar and Robin Jia and Percy Liang},
  title       = {Know What You Don't Know: Unanswerable Questions for SQuAD},
  abstract    = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.},
  date        = {2018-06-11},
  eprint      = {http://arxiv.org/abs/1806.03822v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1806.03822v1:PDF},
  keywords    = {cs.CL},
}

@Article{Seo2016,
  author      = {Minjoon Seo and Aniruddha Kembhavi and Ali Farhadi and Hannaneh Hajishirzi},
  title       = {Bidirectional Attention Flow for Machine Comprehension},
  abstract    = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
  date        = {2016-11-05},
  eprint      = {http://arxiv.org/abs/1611.01603v6},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1611.01603v6:PDF},
  keywords    = {cs.CL},
}

@Article{Bahdanau2014,
  author      = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title       = {Neural Machine Translation by Jointly Learning to Align and Translate},
  abstract    = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  date        = {2014-09-01},
  eprint      = {http://arxiv.org/abs/1409.0473v7},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1409.0473v7:PDF},
  keywords    = {cs.CL, cs.LG, cs.NE, stat.ML},
}

@inproceedings{Gardner2017ADS,
  title={A Deep Semantic Natural Language Processing Platform},
  author={Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord and Pradeep Dasigi and Nelson H S Liu and Matthew Peters and Michael Schmitz and Luke S. Zettlemoyer},
  year={2017}
}

@Comment{jabref-meta: databaseType:bibtex;}
